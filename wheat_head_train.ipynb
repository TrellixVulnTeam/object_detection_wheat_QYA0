{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wheat_head_train.ipynb",
      "provenance": [],
      "mount_file_id": "1bZbKpoiE0ZNijS5pjW1jziNEOPo4m2iw",
      "authorship_tag": "ABX9TyNs3GRl34GDtQSH/6LZdh/V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sk1210/object_detection_wheat/blob/master/wheat_head_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcnKLDsqfkwH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "f23530cd-9e30-4a12-b7bc-158c68ead0dc"
      },
      "source": [
        "!pip install wandb\n",
        "!unzip /content/Dataset/global-wheat-detection.zip  -d /content/Dataset/\n",
        "import glob\n",
        "print (len(glob.glob(\"/content/Dataset/train/*.jpg\"))) # 3422"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/Dataset/global-wheat-detection.zip\n",
            "replace /content/Dataset/test/2fd875eaa.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "3422\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmpoJ_hK-QZm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7ab4f657-4409-4c7d-d800-df778a8370d8"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/19/f8db9eff4b0173adf6dd2e8b0c3d8de0bfe10ec9ed63d247665980d82258/wandb-0.9.4-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 3.3MB/s \n",
            "\u001b[?25hCollecting watchdog>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/06/121302598a4fc01aca942d937f4a2c33430b7181137b35758913a8db10ad/watchdog-0.10.3.tar.gz (94kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 8.5MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/1e/a45320cab182bf1c8656107b3d4c042e659742822fc6bff150d769a984dd/GitPython-3.1.7-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 17.5MB/s \n",
            "\u001b[?25hCollecting gql==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/2a/5461e1fe0026d6eab10571f81d052894328e97f15614abb6a576c65bc82d/sentry_sdk-0.16.2-py2.py3-none-any.whl (109kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 23.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.2MB/s \n",
            "\u001b[?25hCollecting graphql-core<2,>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.6.20)\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.10)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: watchdog, subprocess32, gql, pathtools, graphql-core\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.10.3-cp36-none-any.whl size=73870 sha256=a98167776d58726f7ee455670babfba0c33964cc2a25982a05e4719de2aabcc5\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/1d/38/2c19bb311f67cc7b4d07a2ec5ea36ab1a0a0ea50db994a5bc7\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=b33e6b3fb564948fa029f3cea7f73518f16733de5e9e05ffc4dbe1473347fc47\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=b0260df769b19df44253640291f90018bdb13722b3d91e81338f1e9d7d2fecc0\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8784 sha256=7c95daacd569f6888c6af9afa718ace9502ac9038865df73688c7f39c94c4243\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104650 sha256=ed9ab387c743335fbe8eea777ac0875eadb854a6874ea7fc63d7ca61258891ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\n",
            "Successfully built watchdog subprocess32 gql pathtools graphql-core\n",
            "Installing collected packages: pathtools, watchdog, subprocess32, configparser, smmap, gitdb, GitPython, graphql-core, gql, sentry-sdk, docker-pycreds, shortuuid, wandb\n",
            "Successfully installed GitPython-3.1.7 configparser-5.0.0 docker-pycreds-0.4.0 gitdb-4.0.5 gql-0.2.0 graphql-core-1.1 pathtools-0.1.2 sentry-sdk-0.16.2 shortuuid-1.0.1 smmap-3.0.4 subprocess32-3.5.4 wandb-0.9.4 watchdog-0.10.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17jWneCpjkR2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "8f65156a-40db-4d51-ac4a-651bf919deb0"
      },
      "source": [
        "# connect to gdrive \n",
        "# install utils code\n",
        "\n",
        "!git clone https://github.com/sk1210/object_detection_wheat.git\n",
        "!mkdir /content/Dataset\n",
        "!cp  \"/content/drive/My Drive/Dataset/global-wheat-detection.zip\" /content/Dataset/\n",
        "!unzip /content/Dataset/global-wheat-detection.zip  /content/Dataset/\n",
        "import os\n",
        "os.chdir(\"/content/object_detection_wheat\")\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'object_detection_wheat' already exists and is not an empty directory.\n",
            "mkdir: cannot create directory ‘/content/Dataset’: File exists\n",
            "Archive:  /content/Dataset/global-wheat-detection.zip\n",
            "caution: filename not matched:  /content/Dataset/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxKnwE9Ai33Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a3c2ae51-af95-4782-8ea6-bc1d559c765d"
      },
      "source": [
        "# data loader\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import torch\n",
        "import cv2\n",
        "import csv\n",
        "import os\n",
        "from transforms import ToTensor\n",
        "from torchvision.transforms import functional as F\n",
        "import transforms, utils\n",
        "import sys\n",
        "\n",
        "\n",
        "class WheatData(Dataset):\n",
        "    def __init__(self, csv_path=\"\", mode = \"train\"):\n",
        "        self.csv_path = r\"/content/drive/My Drive/Dataset/train.csv\"\n",
        "        self.images_path = r\"/content/Dataset/train/\"\n",
        "        self.mode = mode\n",
        "        self.transforms = transforms.RandomHorizontalFlip(0.5)\n",
        "        self.images_info = {}\n",
        "        self.total_samples = None\n",
        "        self.image_scale = 604\n",
        "        self.scaleFactor = self.image_scale / 1024  # 0.25\n",
        "\n",
        "        self.read_csv()\n",
        "\n",
        "\n",
        "    def set_image_scale(self, image_scale):\n",
        "        self.image_scale = image_scale\n",
        "        self.scaleFactor = self.image_scale / 1024  # 0.25\n",
        "\n",
        "    def write_images(self, out_dir):\n",
        "        if not os.path.exists(out_dir):\n",
        "            os.makedirs(out_dir)\n",
        "        for key, data in self.images_info.items():\n",
        "            img_path = os.path.join(self.images_path, key + \".jpg\")\n",
        "            boxes = data[\"bbox\"]\n",
        "\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            # draw boxes\n",
        "            for box in boxes:\n",
        "                x1, y1, w, h = list(map(int, box))\n",
        "                cv2.rectangle(img, (x1, y1), (x1 + w, y1 + h), (255, 100, 0), 3)\n",
        "                cv2.imwrite(os.path.join(out_dir, os.path.basename(img_path)), img)\n",
        "\n",
        "    def read_csv(self):\n",
        "        print(\"Loading CSV \")\n",
        "        with open(self.csv_path) as f:\n",
        "            reader = csv.reader(f)\n",
        "            header = next(reader)\n",
        "            for row in reader:\n",
        "                image_id = row[0]\n",
        "                if not (image_id in self.images_info):\n",
        "                    self.images_info[image_id] = {}\n",
        "\n",
        "                for key, value in zip(header, row):\n",
        "                    if key == \"bbox\":\n",
        "                        box = value.replace(\"[\", \"\").replace(\"]\", \"\").replace(\" \", \"\").split(\",\")\n",
        "                        x1, y1, w, h = list(map(float, box))\n",
        "                        box = [x1, y1, x1 + w, y1 + h]\n",
        "                        if key in self.images_info[image_id]:\n",
        "                            self.images_info[image_id][key].append(box)\n",
        "                        else:\n",
        "                            self.images_info[image_id][key] = [box]\n",
        "                    else:\n",
        "                        self.images_info[image_id][key] = value\n",
        "                # print (self.images_info)\n",
        "        self.key_lists = list(self.images_info.keys())\n",
        "        self.key_lists.sort()\n",
        "        self.key_list = self.key_lists[:-100]\n",
        "        self.key_list = self.key_lists\n",
        "        self.total_samples = len(self.key_list)\n",
        "\n",
        "    def apply_transorms(self, img, target):\n",
        "\n",
        "        hor_flip = transforms.RandomHorizontalFlip(0.5)\n",
        "        ver_flip = transforms.RandomVerticalFlip(0.4)\n",
        "        jitter = transforms.ColorJitter(brightness=0.3, contrast=0.2, hue=0.1)\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "            img = jitter(img)\n",
        "        img = F.to_tensor(img)\n",
        "        if self.mode == \"train\":\n",
        "            img, target = hor_flip(img, target)\n",
        "            img, target = ver_flip(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        key = self.key_list[idx]\n",
        "        #print(idx, key)\n",
        "        data = self.images_info[key]\n",
        "        img_path = os.path.join(self.images_path, key + \".jpg\")\n",
        "        boxes = data[\"bbox\"]\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        width, height = int(img.width * self.scaleFactor), int(img.height * self.scaleFactor)\n",
        "        img = img.resize((width, height))\n",
        "        # convert everything into a torch.Tensor\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32) * self.scaleFactor\n",
        "        labels = torch.ones((len(boxes),), dtype=torch.int64)\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n",
        "        if self.mode == \"test\":\n",
        "            target[\"image_id\"] = torch.tensor([idx])\n",
        "            target[\"area\"] = area\n",
        "            target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.apply_transorms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total_samples\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset = WheatData()\n",
        "    print(\"END\")\n",
        "    # image_data.write_images(\"kaggle/temp/output\")\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=10, shuffle=True, collate_fn=utils.collate_fn)\n",
        "    #print(next(iter(dataloader)))\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading CSV \n",
            "END\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMYlxYhgiReH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "import torch\n",
        "from torchvision.models.detection.backbone_utils import IntermediateLayerGetter\n",
        "from collections import OrderedDict\n",
        "from torch import nn\n",
        "from torchvision.ops import misc as misc_nn_ops\n",
        "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork, LastLevelMaxPool\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "from torchvision.models.detection.faster_rcnn import FasterRCNN\n",
        "from torchvision.models import resnext50_32x4d, resnet101, resnext101_32x8d\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "try:\n",
        "    from torch.hub import load_state_dict_from_url\n",
        "except ImportError:\n",
        "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
        "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
        "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
        "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
        "}\n",
        "\n",
        "class BackboneWithFPN(nn.Sequential):\n",
        "    \"\"\"\n",
        "    Adds a FPN on top of a model.\n",
        "\n",
        "    Internally, it uses torchvision.models._utils.IntermediateLayerGetter to\n",
        "    extract a submodel that returns the feature maps specified in return_layers.\n",
        "    The same limitations of IntermediatLayerGetter apply here.\n",
        "\n",
        "    Arguments:\n",
        "        backbone (nn.Module)\n",
        "        return_layers (Dict[name, new_name]): a dict containing the names\n",
        "            of the modules for which the activations will be returned as\n",
        "            the key of the dict, and the value of the dict is the name\n",
        "            of the returned activation (which the user can specify).\n",
        "        in_channels_list (List[int]): number of channels for each feature map\n",
        "            that is returned, in the order they are present in the OrderedDict\n",
        "        out_channels (int): number of channels in the FPN.\n",
        "\n",
        "    Attributes:\n",
        "        out_channels (int): the number of channels in the FPN\n",
        "    \"\"\"\n",
        "    def __init__(self, backbone, return_layers, in_channels_list, out_channels):\n",
        "        body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
        "        fpn = FeaturePyramidNetwork(\n",
        "            in_channels_list=in_channels_list,\n",
        "            out_channels=out_channels,\n",
        "            extra_blocks=LastLevelMaxPool(),\n",
        "        )\n",
        "        super(BackboneWithFPN, self).__init__(OrderedDict(\n",
        "            [(\"body\", body), (\"fpn\", fpn)]))\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "\n",
        "def resnet_fpn_backbone(backbone_name, pretrained):\n",
        "    #backbone = resnet.__dict__[backbone_name](\n",
        "    # backbone = resnext101_32x8d(\n",
        "    #     pretrained=pretrained,\n",
        "    #     norm_layer=misc_nn_ops.FrozenBatchNorm2d)\n",
        "    backbone = torch.hub.load('pytorch/vision:v0.6.0', backbone_name, pretrained=True)\n",
        "    # freeze layers\n",
        "    for name, parameter in backbone.named_parameters():\n",
        "        if 'layer2' not in name and 'layer3' not in name and 'layer4' not in name:\n",
        "            parameter.requires_grad_(False)\n",
        "\n",
        "    return_layers = {'layer1': 0, 'layer2': 1, 'layer3': 2, 'layer4': 3}\n",
        "\n",
        "    in_channels_stage2 = 256\n",
        "    in_channels_list = [\n",
        "        in_channels_stage2,\n",
        "        in_channels_stage2 * 2,\n",
        "        in_channels_stage2 * 4,\n",
        "        in_channels_stage2 * 8,\n",
        "    ]\n",
        "    out_channels = 256\n",
        "    return BackboneWithFPN(backbone, return_layers, in_channels_list, out_channels)\n",
        "\n",
        "def _fasterrcnn_resnext100(pretrained=False, progress=True,\n",
        "                            num_classes=91, pretrained_backbone=True, **kwargs):\n",
        "\n",
        "    backbone = resnet_fpn_backbone('resnext101_32x8d', pretrained_backbone)\n",
        "    model = FasterRCNN(backbone, num_classes, **kwargs)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def faster_rcnn_resnext(num_classes):\n",
        "    # load a model pre-trained pre-trained on COCO\n",
        "\n",
        "    model = _fasterrcnn_resnext100(pretrained=False, pretrained_backbone = True)\n",
        "\n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = faster_rcnn_resnext(2)\n",
        "    print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VHSQR30yiZL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "22a8ecd8-f397-4ec0-d610-451b4b00f832"
      },
      "source": [
        "import torch\n",
        "# from data_loader import WheatData as Dataset\n",
        "#from model import faster_rcnn, faster_rcnn_101, faster_rcnn_custom\n",
        "#from model_custom import faster_rcnn_resnext\n",
        "import utils\n",
        "from engine import train_one_epoch\n",
        "import random\n",
        "\n",
        "weight_dir = r\"/content/drive/My Drive/Dataset/Wheat_head_detection/weights/resnext101/\"\n",
        "\n",
        "weight_file = r\"kaggle\\weights\\custom_anchors\\24.weight\"\n",
        "start_epoch = 0\n",
        "\n",
        "def main():\n",
        "    # train on the GPU or on the CPU, if a GPU is not available\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    # our dataset has two classes only - background and person\n",
        "    num_classes = 2\n",
        "    # use our dataset and defined transformations\n",
        "    dataset = WheatData()\n",
        "    # dataset_test = Dataset()\n",
        "\n",
        "    # split the dataset in train and test set\n",
        "    indices = list(range(len(dataset))) #.randperm(len(dataset)).tolist()\n",
        "    # dataset = torch.utils.data.Subset(dataset, indices[:-100])\n",
        "    # dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
        "\n",
        "    # define training and validation data loaders\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=1, shuffle=True, num_workers=1,\n",
        "        collate_fn=utils.collate_fn)\n",
        "\n",
        "    # data_loader_test = torch.utils.data.DataLoader(\n",
        "    #     dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
        "    #     collate_fn=utils.collate_fn)\n",
        "\n",
        "    # get the model using our helper function\n",
        "    # model = faster_rcnn(num_classes)\n",
        "    # model = faster_rcnn_101(num_classes)\n",
        "    # model = faster_rcnn_custom(num_classes)\n",
        "    model = faster_rcnn_resnext(num_classes)\n",
        "\n",
        "    # move model to the right device\n",
        "    model.to(device)\n",
        "\n",
        "    # construct an optimizer\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                                momentum=0.9, weight_decay=0.0005)\n",
        "    # and a learning rate scheduler\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                   step_size=7,\n",
        "                                                   gamma=0.1)\n",
        "\n",
        "    # let's train it for 10 epochs\n",
        "    num_epochs = 35\n",
        "    print(\"start training\")\n",
        "    #wandb.watch(model)\n",
        "    image_scales = [512, 512, 604, 604, 604]\n",
        "  \n",
        "    #model.load_state_dict(torch.load(weight_file))\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        # train for one epoch, printing every 10 iterations\n",
        "        image_scale = random.choice(image_scales)\n",
        "        dataset.set_image_scale(image_scale)\n",
        "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10, image_scale = image_scale)\n",
        "        # update the learning rate\n",
        "        torch.save(model.state_dict(), weight_dir+str(epoch) + \".weight\")\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # evaluate on the test data set\n",
        "        # evaluate(model, data_loader_test, device=device)\n",
        "\n",
        "    print(\"That's it!\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading CSV \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.6.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "start training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-8d62f01530bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-40-8d62f01530bb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mimage_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_scales\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_image_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;31m# update the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".weight\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: train_one_epoch() got an unexpected keyword argument 'image_scale'"
          ]
        }
      ]
    }
  ]
}